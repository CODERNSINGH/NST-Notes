# â³ Time Complexity â€“ Pre-Read Notes

Time complexity helps us analyze **how efficient an algorithm is** as input size grows.  
We measure **growth rate** (not real seconds) using asymptotic notations.

---

## ğŸ”¹ Level 1 â€“ The Basics

### 1. What is Time Complexity?
It estimates algorithm running time using `O(...)` notation.

Example:
- Print "Hello" â†’ **O(1)**
- Print numbers from 1 to `n` â†’ **O(n)**

---

### 2. Loops
- One loop â†’ **O(n)**
- Two nested loops â†’ **O(nÂ²)**
- k nested loops â†’ **O(náµ)**

---

### 3. Constant Factors
We ignore constants because they donâ€™t change growth:
- `3n` â†’ O(n)
- `n/2` â†’ O(n)

---

### 4. Multiple Phases
The slowest dominates:
- O(n) + O(nÂ²) â†’ O(nÂ²)

---

### 5. Recursive Functions
- Fibonacci (two recursive calls) â†’ **O(2â¿)**
- Merge Sort â†’ **O(n log n)**

---

### 6. Common Complexities (Fast â†’ Slow)
- **O(1)** â†’ constant (array access)
- **O(log n)** â†’ logarithmic (binary search)
- **O(âˆšn)** â†’ square root
- **O(n)** â†’ linear (loop over array)
- **O(n log n)** â†’ sorting (merge sort, quicksort avg.)
- **O(nÂ²)** â†’ quadratic (bubble sort, two loops)
- **O(nÂ³)** â†’ cubic (three nested loops)
- **O(2â¿)** â†’ exponential (subsets)
- **O(n!)** â†’ factorial (permutations)

ğŸ‘‰ Rule of thumb:
- `n â‰¤ 10Â³` â†’ O(nÂ²) is okay
- `n â‰¤ 10âµ` â†’ O(n) or O(n log n)

---

## ğŸ”¹ Level 2 â€“ Divide & Conquer + Recurrences

### Divide & Conquer
Steps:
1. **Divide** problem into smaller subproblems.
2. **Conquer** solve them recursively.
3. **Combine** merge results.

Example: Merge Sort  
`T(n) = 2T(n/2) + O(n)` â†’ **O(n log n)**

---

### Solving Recurrences
1. **Recursion Tree Method** â†’ expand tree, sum levels.
2. **Master Theorem** â†’ quick formula for recurrences like:  
   `T(n) = aT(n/b) + f(n)`.

---

## ğŸ”¹ Level 3 â€“ Asymptotic Notations

- **Î˜ (Theta):** exact/tight bound.  
  Example: Bubble Sort = Î˜(nÂ²)

- **O (Big O):** upper bound.  
  Example: Bubble Sort = O(nÂ²)

- **Î© (Omega):** lower bound.  
  Example: Bubble Sort = Î©(n)

- **o (little o):** strictly smaller.  
  Example: f(n) = o(nÂ²) â†’ grows slower than nÂ²

- **Ï‰ (little omega):** strictly larger.  
  Example: f(n) = Ï‰(n) â†’ grows faster than n

---

# Asymptotic Notations in Detail: Î˜, O, Î©, o, Ï‰

We compare two non-negative functions of input size, `f(n)` (your algorithmâ€™s cost) and `g(n)` (a reference growth rate).

> Tip: Read â€œeventuallyâ€ as â€œfor all sufficiently large `n`â€.

---

## 1) Big-Theta: **Î˜(g(n))** â€” *Tight bound*

**Intuition:** `f` grows at the **same rate** as `g` (within constant factors).  
**Meaning:** `g` is both an upper and a lower bound on `f`.

**Formal definition:**  
`f(n) = Î˜(g(n))` if âˆƒ constants `c1, c2 > 0` and `n0` such that  
`c1Â·g(n) â‰¤ f(n) â‰¤ c2Â·g(n)` for all `n â‰¥ n0`.

**Examples:**
- `f(n) = 3n + 7` is `Î˜(n)`. (Choose `c1=1`, `c2=4`, for large `n`.)
- `f(n) = n^2 + 100n` is `Î˜(n^2)`.

**How to prove Î˜:**
1. Prove `f(n) = O(g(n))` (upper bound).
2. Prove `f(n) = Î©(g(n))` (lower bound).
If both hold, you have `Î˜(g(n))`.

---

## 2) Big-Oh: **O(g(n))** â€” *Asymptotic upper bound*

**Intuition:** `f` never grows **faster** than a constant multiple of `g`, eventually.

**Formal definition:**  
`f(n) = O(g(n))` if âˆƒ constants `c > 0` and `n0` such that  
`f(n) â‰¤ cÂ·g(n)` for all `n â‰¥ n0`.

**Examples:**
- `3n + 7 = O(n)` (take `c=4`, `n0=7`).
- `n log n = O(n^1.1)` (since `log n = o(n^0.1)`).
- Bubble sort (worst-case) time `n^2` â‡’ `T(n) = O(n^2)`.

**Limit test (handy):**  
If `limsup_{nâ†’âˆ} f(n)/g(n) < âˆ`, then `f(n) = O(g(n))`.

**Common pitfall:** Writing `f(n) = O(n)` as equality. Itâ€™s set membership: better is `f(n) âˆˆ O(n)` (but CS uses `=` informally).

---

## 3) Big-Omega: **Î©(g(n))** â€” *Asymptotic lower bound*

**Intuition:** `f` never grows **slower** than a constant multiple of `g`, eventually.

**Formal definition:**  
`f(n) = Î©(g(n))` if âˆƒ constants `c > 0` and `n0` such that  
`f(n) â‰¥ cÂ·g(n)` for all `n â‰¥ n0`.

**Examples:**
- `3n + 7 = Î©(n)` (take `c=3/2`, for large `n`).
- Any algorithm that must read all `n` items has `Î©(n)` time.

**Limit test:**  
If `liminf_{nâ†’âˆ} f(n)/g(n) > 0`, then `f(n) = Î©(g(n))`.

---

## 4) Little-oh: **o(g(n))** â€” *Strictly smaller order*

**Intuition:** `f` grows **strictly slower** than `g` (no constant multiple of `g` can upper-bound `f` tightly).

**Formal definition:**  
`f(n) = o(g(n))` if for **every** `c > 0` âˆƒ `n0` such that  
`f(n) â‰¤ cÂ·g(n)` for all `n â‰¥ n0`.  
Equivalent: `lim_{nâ†’âˆ} f(n)/g(n) = 0`.

**Examples:**
- `log n = o(n^Îµ)` for any `Îµ > 0`.
- `n = o(n log n)` (ratio `1/log n â†’ 0`).
- `n^2 = o(n^2 log n)`.

**Counterexample:**  
`3n + 7` is **not** `o(n)` because `(3n+7)/n â†’ 3 â‰  0`.

---

## 5) Little-omega: **Ï‰(g(n))** â€” *Strictly larger order*

**Intuition:** `f` grows **strictly faster** than `g`.

**Formal definition:**  
`f(n) = Ï‰(g(n))` if for **every** `c > 0` âˆƒ `n0` such that  
`f(n) â‰¥ cÂ·g(n)` for all `n â‰¥ n0`.  
Equivalent: `lim_{nâ†’âˆ} f(n)/g(n) = âˆ`.

**Examples:**
- `n = Ï‰(log n)`.
- `n^1.001 = Ï‰(n)`? **No** (ratio â†’ âˆ would be needed). Instead, `n^1.001 = Ï‰(n^1)` is true (ratio `n^0.001 â†’ âˆ`).
- `2^n = Ï‰(n^k)` for any fixed `k`.

---

## Relationships & Quick Facts

- **Set relations:**  
  `Î˜(g) = O(g) âˆ© Î©(g)`  
  `o(g) âŠ‚ O(g)` and `Ï‰(g) âŠ‚ Î©(g)` (strict subsets).

- **Limits summary:**  
  - `f = Î˜(g)` â‡” `f/g â†’ c` with `0 < c < âˆ`.  
  - `f = O(g)` â‡” `limsup f/g < âˆ`.  
  - `f = Î©(g)` â‡” `liminf f/g > 0`.  
  - `f = o(g)` â‡” `f/g â†’ 0`.  
  - `f = Ï‰(g)` â‡” `f/g â†’ âˆ`.

- **Max rule (very useful):**  
  `f(n) + g(n) = Î˜(max{f(n), g(n)})`.

- **Product rule:**  
  If `f âˆˆ O(h)` and `g âˆˆ O(k)`, then `fÂ·g âˆˆ O(hÂ·k)` (similarly for Î©, Î˜ under positivity).

- **Scaling:**  
  Multiplying by positive constants doesnâ€™t change the class:  
  `cÂ·f(n) âˆˆ Î˜(f(n))` for `c > 0`.

- **Monotone comparison:**  
  If `g1(n) â‰¤ cÂ·g2(n)` eventually, then `O(g1) âŠ† O(g2)`.

---

## Worked Micro-Proofs

**A) Show `3n + 7 = Î˜(n)`**
- Upper bound: for `n â‰¥ 7`, `3n + 7 â‰¤ 3n + n = 4n` â‡’ `O(n)`.
- Lower bound: for all `n â‰¥ 1`, `3n + 7 â‰¥ 3n â‰¥ (3/4)Â·4n` â‡’ `Î©(n)`.
- Both hold â‡’ `Î˜(n)`.

**B) Show `n log n = o(n^{1+Îµ})` (any `Îµ>0`)**
- `(n log n) / n^{1+Îµ} = (log n) / n^{Îµ} â†’ 0` (polynomial beats log).  
- Limit 0 â‡’ little-oh.

**C) Show `n^2 + 100n = o(n^2 log n)`**
- `(n^2 + 100n) / (n^2 log n) = 1/log n + 100/(n log n) â†’ 0`.  
- Hence `o(n^2 log n)`.

**D) Show `2^n = Ï‰(n^k)` for any fixed `k`**
- `(2^n) / (n^k) â†’ âˆ` (exponential beats polynomial).  
- Hence `Ï‰(n^k)`.

---

## Choosing the Right Notation

- Want a **tight, precise** growth rate? Use **Î˜**.  
- Only know an **upper bound** (e.g., worst case)? Use **O**.  
- Only know a **lower bound** (e.g., input must be read)? Use **Î©**.  
- Want to say **strictly slower**? Use **o**.  
- Want to say **strictly faster**? Use **Ï‰**.

---

## Common Pitfalls

- **Confusing O with Î˜:**  
  Saying `T(n) = O(n^2)` doesnâ€™t mean itâ€™s *exactly* `n^2`; it could be `n log n`. `Î˜` is the tight claim.

- **Ignoring sign restrictions:**  
  These are for **non-negative** functions eventually. If your function can be negative, compare absolute values or ensure eventual positivity.

- **Mixing average/worst/best case:**  
  Notation is about **bounds**; you still need to specify which case (best/avg/worst) youâ€™re bounding.

---

## Quick Growth Ladder (from slower to faster)

`1 â‰º log n â‰º n^Î± â‰º n^Î± log^k n â‰º n^Î² (Î±<Î²) â‰º c^n (c>1) â‰º n!`

Use this to quickly decide `o/Ï‰/Î˜` relationships.

---

## Tiny Practice (answers by limit or constants)

1. Is `n^1.5` in `o(n^1.6)`?  
2. Is `n log n` in `Ï‰(n)`?  
3. Is `log^2 n` in `Î˜(log n)`?  
4. Is `n^2` in `O(n^2 / log n)`?  
5. Is `n / log n` in `o(n^0.9)`?



---

## ğŸ”¹ NP-Complete Problems

- Special class of very **hard problems**.  
- If one NP-complete problem is solved in polynomial time, **all can be solved**.  
- Example: Subset Sum, Travelling Salesman.  
- In practice â†’ no efficient solution known.

---

## ğŸ”¹ Importance of Constants

Asymptotic complexity hides constants, but in practice:
- Big constants can slow O(n).
- Small constants can speed up O(n log n).

**Optimizations:**
- Precompute values
- Avoid repeated loops
- Pass by reference
- Reuse memory

---

## ğŸ“ Quick Reference Table

| Complexity | Name           | Example                     |
|------------|---------------|-----------------------------|
| O(1)       | Constant      | Access array element        |
| O(log n)   | Logarithmic   | Binary Search               |
| O(âˆšn)      | Root          | Trial division for primes   |
| O(n)       | Linear        | Loop through array          |
| O(n log n) | Log-linear    | Merge Sort, QuickSort (avg) |
| O(nÂ²)      | Quadratic     | Bubble Sort, Matrix mult    |
| O(nÂ³)      | Cubic         | 3D matrix ops               |
| O(2â¿)      | Exponential   | Subsets, recursion-heavy    |
| O(n!)      | Factorial     | Permutations, Travelling Salesman |

---
âœ… **Flow for Learning:**
1. Basics (O(1) â€¦ O(n!))
2. Recursion + Divide & Conquer
3. Notations (O, Î˜, Î©, o, Ï‰)
4. NP-complete awareness
5. Apply based on input constraints
